{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0, w1 is: [68.19905213 -1.01895735]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# practice II\n",
    "X = np.array([5,6,12,9,15,16])\n",
    "y = np.array([64,56,50,71,44,60])\n",
    "\n",
    "X_tilde = np.c_[np.ones(X.shape[0],dtype =int), X]\n",
    "w = np.linalg.pinv(X_tilde) @ y\n",
    "print(\"w0, w1 is:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.081, 4.321]), array([2.141291, 4.39069 ]), array([2.19682978, 4.40312944])]\n"
     ]
    }
   ],
   "source": [
    "# Practice III\n",
    "w = np.array([2,4]).T\n",
    "\n",
    "# Initialise arrays to store weights at each iteration\n",
    "w_all = []\n",
    "learning_rate = 0.001\n",
    "\n",
    "# GD update of weights\n",
    "for i in range(3):\n",
    "    # cost and gradient descent of the linear least squares model\n",
    "    gd = (X_tilde.T @ X_tilde @ w) - (X_tilde.T @ y)\n",
    "    cost = np.sum(np.square((y- X_tilde@w)))\n",
    "\n",
    "    # Weight update\n",
    "    w = w - learning_rate * gd\n",
    "    \n",
    "    # save w of each iteration in w_all  \n",
    "    w_all.append(w)    \n",
    "\n",
    "print(w_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
