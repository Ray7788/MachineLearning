{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  COMP24112 Lab 2: News Article Classification by k-NN\n",
    "\n",
    "## 1. Task description\n",
    "\n",
    "You will work on a news article classification task.\n",
    "The provided dataset includes a total of 800 articles taken from Reuters newswire.\n",
    "They belong to 4 classes: \"earn\" (0), \"crude\" (1), \"trade\" (2) and \"interest\" (3).\n",
    "There are 200 articles per class.\n",
    "Each article is characterised by word occurrences.\n",
    "The list of used words is called a vocabulary.\n",
    "In our dataset, the vocabulary includes a total of 6428 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "\n",
    "First we need to import the data.\n",
    "Run the below cell to load the data using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "\n",
    "# data, (0, 1, 2, 3), (earn, crude, trade, interest)\n",
    "data, labels, class_names, vocabulary = np.load(\"ReutersNews_4Classes_sparse.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Sparsity\n",
    "\n",
    "Most documents only contain a small subset of the vocabulary, resulting in a very sparse data matrix.\n",
    "To handle the sparsity, in this exercise `data` is represented as a `scipy.sparse.csr_matrix`, which can store sparse matrices efficiently while still allowing efficient row-based indexing.\n",
    "You can learn more about `csr_matrix` and other ways of dealing with sparse matrices at https://docs.scipy.org/doc/scipy/reference/sparse.html.\n",
    "\n",
    "Note, however, that `data` is **not** a normal NumPy array.\n",
    "While most operations will be the same as with a normal dense array, **you cannot use a sparse matrix to index another matrix**.\n",
    "If you need to do this, either first convert the matrix to a NumPy array with the `toarray()` method, or use methods specifically designed to work with sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[41,:]) # A sparse row vector; the output will be the non-zero indices and their values.\n",
    "print(data[41,:].toarray()) # Convert back to a NumPy array. Note that the result is a (1, 6428) matrix, not a vector.\n",
    "# print(vocabulary[data[41,:] > 0]) # Can't index vocabulary with a sparse matrix.\n",
    "rows, columns, values = scipy.sparse.find(data[41,:]) # Find the non-zero entries in the 42nd document.\n",
    "print(vocabulary[columns]) # Prints the words present in the 42nd document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full vocabulary, you can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\", \".join(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how many times article $i$ contains word $j$ using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = 40, 2\n",
    "print(data[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see which class the $i$th article belongs to using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Occurrences:\", data[0,10])\n",
    "print(\"Class:\", class_names[labels[0]])\n",
    "print(\"Word:\", vocabulary[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that the 11th word appears twice in the first document, the first document belongs to the class \"earn\", and the 11th word is \"shareholder\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function randomly selects a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_indices(labels, *num_per_class):\n",
    "    \"\"\"\n",
    "    Returns randomly selected indices. It will return the specified number of indices for each class.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for cls, num in enumerate(num_per_class):\n",
    "        cls_indices = np.where(labels == cls)[0]\n",
    "        indices.extend(np.random.choice(cls_indices, size=num, replace=False))\n",
    "    return np.array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to get one sample from the first class, two from the second, three from the third, and four from the fourth, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = sample_indices(labels, 1, 2, 3, 4)\n",
    "print(\"Returned indices:\", indices)\n",
    "print(\"Samples:\", data[indices])\n",
    "print(\"Corresponding classes:\", labels[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-NN Implementation (4 Marks, Normal)\n",
    "\n",
    "Now, you will need to implement a k-NN classifier by filling the code below.\n",
    "This function should support two types of distance measures: Euclidean distance and cosine distance (defined as 1 - cosine similarity). It should take a set of training samples, a user-specified neighbour number, a distance option, and features of a set of testing samples as the input.\n",
    "It should return the predicted classes for the input set of testing samples.\n",
    "\n",
    "In order to get 4 marks, you are asked to implement the k-NN classifier from scrach without relying on any machine learning library, particularly the distance calculation. But you are allowed to research NumPy functions relating to sorting. If you decide to use existing distance implementation from libraries, e.g., `sklearn.metrics.pairwise_distances` imported as `cdist`, you can get at most 3 marks.\n",
    "\n",
    "**Your implementation must NOT make use of Python loops over individual samples or features**.\n",
    "You should use functions that operate on whole matrices, as this will be much faster than looping in Python.\n",
    "Each experiment below is expected to take no more than 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(A, B, metric, squared=False):\n",
    "    \"\"\"\n",
    "    Compute all pairwise distances between vectors in A and B.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np.array\n",
    "        shape should be (M, K)\n",
    "    B : np.array\n",
    "        shape should be (N, K)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    D : np.array\n",
    "        A matrix D of shape (M, N).  Each entry in D i,j represnets the\n",
    "        distance between row i in A and row j in B.\n",
    "    \"\"\"\n",
    "    \n",
    "    if metric == \"euclidean\":\n",
    "        # Check if two matrices are matched         \n",
    "        assert A.shape[1] == B.shape[1], f\"The number of components for vectors in A \\\n",
    "            {A.shape[1]} does not match that of B {B.shape[1]}!\"\n",
    "\n",
    "#         D_squared = np.sqrt(np.sum(np.square(A)[:,np.newaxis,:], axis=2) - 2 * A.dot(B.T) + np.sum(np.square(B), axis=1))\n",
    "\n",
    "        D_squared = np.sqrt(np.sum(np.square(A.toarray())[:,np.newaxis,:], axis=2) - 2 * A.toarray().dot(B.toarray().T) + np.sum(np.square(B.toarray()), axis=1))\n",
    "    elif metric == \"cosine\":\n",
    "        # Calculate the norms of each row of A and B\n",
    "#         norms_A = np.sqrt(np.sum(A ** 2, axis=1))\n",
    "#         norms_B = np.sqrt(np.sum(B ** 2, axis=1))\n",
    "        norms_A = np.sqrt(np.sum(A.power(2), axis=1))\n",
    "        norms_B = np.sqrt(np.sum(B.power(2), axis=1))  \n",
    "\n",
    "\n",
    "#         print(norms_A.shape, norms_B.shape)\n",
    "        # Calculate the outer product of the norms of A and B\n",
    "#         outer_product = norms_A[:, np.newaxis] * norms_B[np.newaxis, :]\n",
    "        outer_product = norms_A.dot(norms_B.T)\n",
    "    \n",
    "#         D_squared = A.dot(B.T) / outer_product\n",
    "        D_squared = 1 - (A@B.T / outer_product)\n",
    "\n",
    "                            \n",
    "    return D_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# from sklearn.metrics.pairwise import pairwise_distances as cdist\n",
    "\n",
    "def knn_classify(test_samples, training_data, training_labels, metric=\"euclidean\", k=1):\n",
    "    \"\"\"\n",
    "    Performs k-nearest neighbour classification on the provided samples,\n",
    "    given training data and the corresponding labels.\n",
    "    \n",
    "    test_samples: An m x d matrix of m samples to classify, each with d features.\n",
    "    training_data: An n x d matrix consisting of n training samples, each with d features.\n",
    "    training_labels: A vector of size n, where training_labels[i] is the label of training_data[i].\n",
    "    metric: The metric to use for calculating distances between samples.\n",
    "    k: The number of nearest neighbours to use for classification.\n",
    "    \n",
    "    Returns: A vector of size m, where out[i] is the predicted class of test_samples[i].\n",
    "    \"\"\"\n",
    "    # Calculate an m x n distance matrix.\n",
    "#     test_samples = test_samples.toarray()\n",
    "#     training_data = training_data.toarray()\n",
    "    pairwise_distance = distance_matrix(test_samples, training_data, metric=metric)\n",
    "    \n",
    "    # Find the k nearest neighbours of each samples as an m x k matrix of indices.\n",
    "    if metric == \"euclidean\":\n",
    "        # larger means more similar         \n",
    "#         nearest_neighbours = np.argsort(pairwise_distance, axis=1)[:, :k]\n",
    "        nearest_neighbours = np.argpartition(pairwise_distance, k)[:, :k]\n",
    "    elif metric == \"cosine\":\n",
    "        # smaller means more similar \n",
    "#         nearest_neighbours = np.argsort(pairwise_distance, axis=1)[:, -k:]\n",
    "        nearest_neighbours = np.argpartition(pairwise_distance, k)[:, :k]\n",
    "\n",
    "    \n",
    "    # Look up the classes corresponding to each index.\n",
    "    nearest_labels = np.array([training_labels[nearest_neighbours[i]] for i in range(len(nearest_neighbours))])\n",
    "    \n",
    "    # Return the most frequent class on each row.\n",
    "    # Note: Ensure that the returned vector does not contain any empty dimensions.\n",
    "    # You may find the squeeze method useful here.\n",
    "    return np.squeeze(scipy.stats.mode(nearest_labels, axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2 kinds of data and 2 kinds of labels according to how many articles per class for training and test\n",
    "def choose_data(data, labels, *num_per_class):\n",
    "    training_index = sample_indices(labels, *num_per_class)\n",
    "    training_data = data[training_index]\n",
    "    training_labels = labels[training_index]\n",
    "    \n",
    "    test_index = [i for i in range(len(labels)) if i not in training_index]\n",
    "    test_data = data[test_index]\n",
    "    test_labels = labels[test_index]\n",
    "    return training_data, training_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments (13 Marks in Total)\n",
    "\n",
    "Use your k-NN function to perform the following experiments.\n",
    "\n",
    "### Experiment 1 (3 Marks, Easy)\n",
    "\n",
    "Randomly select 80 articles per class for training, and use the remaining articles for testing.\n",
    "Fix a neighbour number setting as you see fit. Perform k-NN classification using the Euclidean distance and test it.\n",
    "\n",
    "Repeat this process 20 times (trials).\n",
    "Calculate the mean and standard deviation of the testing accuracies. Print out the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_perform1(times, metric, k, *num_per_class):\n",
    "    # The accuracy of each time saves in a list    \n",
    "    accuracy = []\n",
    "    for i in range(times):\n",
    "        training_data, training_labels, test_data, test_labels = choose_data(data, labels, *num_per_class)      \n",
    "        predicted_class = knn_classify(test_data, training_data, training_labels, metric, k)\n",
    "        \n",
    "        \n",
    "        # Compare with test labels then decide if it is consist        \n",
    "#         accuracy.append(np.sum(predicted_class == test_labels) / len(predicted_class))\n",
    "#         accuracy.append(np.count_nonzero(np.equal(test_labels, predicted_class)) / len(test_labels))\n",
    "#     .toarray()\n",
    "        print(predicted_class, \"\\nlebel \\n\", test_labels)\n",
    "        print(predicted_class.shape, test_labels.shape, np.count_nonzero(np.equal(test_labels, predicted_class)), len(test_labels))\n",
    "    return np.array(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_euclidean = knn_perform1(20, \"euclidean\", 5, 80, 80, 80, 80)\n",
    "print(\"mean of the testing accuracies with Euclidean distance:\", np.mean(accuracy_euclidean))\n",
    "print(\"standard deviation(sd.) of the testing accuracies with Euclidean distance:\", np.std(accuracy_euclidean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same neighbour number, but use the cosine distance instead of the Euclidean distance.\n",
    "Repeat the same experiment.\n",
    "\n",
    "Print out the mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_cosine = knn_perform1(20, \"cosine\", 5, 80, 80, 80, 80)\n",
    "print(\"mean of the testing accuracies with cosine distance:\", np.mean(accuracy_cosine))\n",
    "print(\"standard deviation(sd.) of the testing accuracies with cosine distance:\", np.std(accuracy_cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in your report which distance measure gives better performance and analyse the reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# train_data = data[100::,:].toarray()\n",
    "# train_label = labels[100::]\n",
    "# test_sample = data[:100,:].toarray()\n",
    "# print(knn_classify(test_sample,train_data,train_label,k=5))\n",
    "# print(knn_classify(test_sample,train_data,train_label,'cosine',k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 (5 Marks, Easy)\n",
    "\n",
    "Using the distance measure that you found performs better in Experiment 1.\n",
    "\n",
    "Randomly select 80 articles per class for training, and use the remaining articles for testing. Perform k-NN classification with the neighbour number $k$ varying from 1 to 50.\n",
    "\n",
    "For each values of $k$, repeat the training process by 20 trials and record the average training error rates and standard deviation.\n",
    "\n",
    "Do the same for testing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_perform2(times, metric, *nums):\n",
    "    average_training_errors, average_test_errors = [], []\n",
    "    sd_training_errors, sd_test_errors = [], []\n",
    "    for k in range(1, 51):\n",
    "        train_err, test_err = [], []\n",
    "        for _ in range(times):\n",
    "            training_data, training_labels, test_data, test_labels = choose_data(data, labels, *nums)\n",
    "            predicted_class = knn_classify(training_data, training_data, training_labels, metric, k)\n",
    "            train_err.append(np.sum(predicted_class != training_labels) / len(predicted_class))\n",
    "            predicted_class = knn_classify(test_data, training_data, training_labels, metric, k)\n",
    "            test_err.append(np.sum(predicted_class != test_labels) / len(predicted_class))\n",
    "            \n",
    "        # Record the average training error rates    \n",
    "        average_training_errors.append(np.mean(np.array(train_err)))\n",
    "        average_test_errors.append(np.mean(np.array(test_err)))\n",
    "        # Record the standard deviation         \n",
    "        sd_training_errors.append(np.std(np.array(train_err)))\n",
    "        sd_test_errors.append(np.std(np.array(test_err)))\n",
    "        \n",
    "    return 0\n",
    "\n",
    "#     return average_training_errors, sd_training_errors, average_test_errors, sd_test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to Experiment1's result, \"cosine\" performs better\n",
    "# better_metric = \"cosine\" if np.mean(accuracy_cosine) > np.mean(accuracy_euclidean) else \"euclidean\"\n",
    "better_metric = \"cosine\"\n",
    "# Randomly select 80 articles per class for training,\n",
    "average_training_errors, sd_training_errors, average_test_errors, sd_test_errors = knn_perform2(20, better_metric, 80, 80, 80, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce an error bar plot showing the training error rate for each $k$ here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.errorbar(range(1, 51), average_training_errors, yerr = sd_training_errors, marker = \"o\", color = 'orange')\n",
    "plt.xlabel(\"k-value\")\n",
    "plt.ylabel(\"average training error rate\")\n",
    "plt.title(\"Average training error rates for different k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce your testing error bar plot here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.errorbar(range(1, 51), average_test_errors, yerr = sd_test_errors, marker = \"o\", color = 'pink')\n",
    "plt.xlabel(\"k-value\")\n",
    "plt.ylabel(\"average testing error rate\")\n",
    "plt.title(\"Average testing error rates for different k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that all graphs should have axis labels and a title.**\n",
    "\n",
    "Discuss in your report the difference between the training and testing accuracies, and why this is the case. \n",
    "\n",
    "Analyse in your report the effect of $k$ based on this experiment. What do you think is a reasonable value for $k$? Comment specifically on the *bias* and *variance* of your model at small and large values of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 (5 Marks, Hard)\n",
    "\n",
    "In this experiment we will create confusion matrices for a more detailed view on our model's performance. Then, we will observe the behaviour of our knn classifier on novel classes.\n",
    "\n",
    "First, randomly select 100 articles per class for training, and use the remaining articles for testing. Set the neighbour number to $k=3$. Perform 3-NN classification using the Cosine distance, as in previous experiments.\n",
    "\n",
    "#### Confusion Matrix Implementation  \n",
    "\n",
    "Implement a multi-class confusion matrix yourself, from scratch. Let the row index correspond to the known label, and column index to predicted label. If you decide to use existing confusion matrix implementation from libraries, e.g., `sklearn.metrics.confusion_matrix`, you can get at most 4 marks. (However, you may use an existing implementation to check the output of your own function.)\n",
    "\n",
    "Print out the confusion matrix and overall accuracy of your classifier for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 100 articles per class for training, and use the remaining articles for testing.\n",
    "training_data3, training_labels3, test_data3, test_labels3 = choose_data(data, labels, 100, 100, 100, 100)\n",
    "y_true = test_labels3\n",
    "y_pred = knn_classify(test_data3, training_data3, training_labels3, \"cosine\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_scratch(y_true, y_pred):\n",
    "    # Get the set of unique labels\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    labels_len = len(labels)\n",
    "    # Initialize the confusion matrix as a 2D numpy array\n",
    "    matrix = np.zeros((labels_len, labels_len), dtype=int)\n",
    "    # Fill in the matrix\n",
    "    for i in range(len(y_true)):\n",
    "#         true_label_index = labels.index(y_true[i])\n",
    "#         pred_label_index = labels.index(y_pred[i])\n",
    "        matrix[y_true[i]][y_pred[i]] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a multi-class confusion matrix myself\n",
    "confusion_matrix_scratch(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use existing confusion matrix implementation from libraries,\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(y_pred == y_true) / len(y_pred)\n",
    "print(\"overall accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Novel Classes\n",
    "\n",
    "5 new articles have been provided in string format below. The code to create a sparse representation of these articles has also been provided. Take a moment to skim through the articles.\n",
    "\n",
    "Run the code below, saving the sparse matrix representation of these 5 articles into `new_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp0 = \"\"\"World number four Jessica Pegula said she thought about ending her tennis career prematurely last year due to her mother Kim's health issues.\n",
    "Kim, the co-owner and president of the NFL's Buffalo Bills and NHL's Buffalo Sabres, suffered a cardiac arrest in June and needed CPR from her other daughter Kelly before paramedics arrived and restored her heartbeat.\n",
    "Pegula received the news after returning home to Florida from the French Open, where she lost to eventual champion Iga Swiatek but rose to number eight in the world.\n",
    "\"Suddenly I went from, 'Let's celebrate top 10 in the world' to, 'Do I need to start thinking about my career after tennis a lot sooner than I thought?'\" Pegula wrote in an essay in The Players' Tribune.\n",
    "\"I'm 28 and I take pride in being able to handle every situation thrown at me, but this was a lot.\"\n",
    "Pegula said she wanted to share her mother's story after Bills safety Damar Hamlin suffered a cardiac arrest during an NFL game last month.\n",
    "Pegula went on to play Wimbledon and the U.S. Open last year to reach a career-high ranking of number three.\n",
    "\"I still wanted to play Wimbledon if I knew my mom was O.K.,\" Pegula wrote. \"My dad didn't want me to play, but I knew she would be upset if I skipped because of her.\n",
    "\"I had to deal with a lot of speculation and questions surrounding her health, even shutting down rumours that she had died,\" added Pegula, who lost to Victoria Azarenka in the quarter-finals of this year's Australian Open.\n",
    "\"It wasn't necessarily the most fun Wimbledon experience I remember. I had a few good wins, and I was proud I was able to go out and compete considering the situation.\" \"\"\"\n",
    "\n",
    "sp1 = \"\"\"Juventus outclassed Salernitana 3-0 on Tuesday in Serie A, with two goals and one assist from striker Dusan Vlahovic helping the visitors move up to 10th place in the standings. The game marked a return to form for Serbian Vlahovic, who has struggled with injuries this season, but made his first league start since October. \n",
    "\"You can see physically, he just moves better, looks sharper, he also played well on a technical level today,\" Juventus manager Massimiliano Allegri told DAZN.\n",
    "Juventus got a penalty after 26 minutes when Hans Nicolussi fouled Manuel Locatelli inside the box with Vlahovic converting the penalty.\n",
    "Vlahovic almost netted a second in the 37th minute, but his shot from an acute angle at the edge of the box went just wide of the post.\n",
    "Filip Kostic doubled the lead on the stroke of halftime when he tapped the ball in from close range after Vlahovic's initial shot bounced into his path, providing an unintended assist.\n",
    "Juventus could have scored a third in the last seconds before the break when Locatelli made a run unmarked into the box, but Salernitana keeper Guillermo Ochoa reacted early and parried his attempted lob.\n",
    "Vlahovic got his second goal 80 seconds into the second half when he ran through in the box and smashed the ball low into the right corner.\n",
    "Salernitana almost pulled one back in the 51st minute, with Junior Sambia sending a cross that went through almost everyone in the box, but forward Boulaye Dia was unable to stretch himself in time to guide the ball into the open net.\n",
    "\"The team gave a strong response, we had a good 60 minutes, but got a bit complacent after going 3-0 up and allowed too many shots on goal. We were static in our positions, didn't move around enough and the players know we must absolutely do better,\" Allegri said.\n",
    "\"The first 10 minutes we tended to pass it too much down the right, so we need to improve our passing, be smoother and keep it simple.\"\n",
    "Juve could have added to their tally but were denied by the woodwork with Angel Di Maria hitting the crossbar after 53 minutes and Moise Kean striking the post late on.\n",
    "The result moved Juventus on to 26 points from 21 matches, while Salernitana are 16th with 21 points.\" \"\"\"\n",
    "\n",
    "sp2 = \"\"\"Manchester United manager Erik ten Hag said he has a long-term plan to build a culture and to develop players at the club.\n",
    "United appointed Ten Hag in April 2022 to succeed interim boss Ralf Rangnick.\n",
    "The team sit third in the Premier League, eight points behind leaders Arsenal, and have the chance to win their first trophy since 2017 when they face Newcastle United in the League Cup final on Feb. 26.\n",
    "\"I always think about the long term, in every club where I was, I have been thinking about long-term work to build a culture, to build a way of playing, to develop the players and the team, obviously,\" Ten Hag told reporters.\n",
    "\"I think in the long term obviously in contracts and in (transfer) windows because I think that is the (right) way.\n",
    "\"I am not here for one year, I am (here for) longer, I see it is a long-term project to build here and how long it is you can't see, I can't tell,\" he added.\"\"\"\n",
    "\n",
    "sp3 = \"\"\"A near-historic Philadelphia Eagles pass rush will face the ultimate test on Sunday in Kansas City Chiefs quarterback Patrick Mahomes, an MVP favourite with no interest in ceding the Super Bowl spotlight.\n",
    "The Eagles established themselves as a terrifying defensive force in the regular season, punishing opponents with an astonishing 70 sacks, two shy of the NFL record, while allowing the second-fewest yards per game.\n",
    "But Mahomes is unlike any quarterback they faced in 2022.\n",
    "\"Mahomes is the guy that extends the plays and drops the dimes,\" defensive end Brandon Graham, who helped the Eagles to the Lombardi Trophy five years ago, told reporters on Tuesday.\n",
    "\"You've got to make sure you can hit him, get him on the ground, create turnovers, make him make bad throws.\"\n",
    "At just 27-years-old Mahomes has already vaulted himself into the history books, joining future Hall of Famer Drew Brees this year as one of only two quarterbacks to throw for more than 5,000 yards and 40 or more touchdowns in multiple seasons.\n",
    "Eagles linebacker Haason Reddick produced a career-best 16 sacks this season but had few answers when asked how the Eagles could contain Mahomes.\n",
    "\"When it comes to Patrick Mahomes, man, he's a tremendous talent,\" he told reporters this week.\n",
    "\"I don't know if you can contain him - I just don't know, he's that good. I won't lie, he is.\"\n",
    "Not even injury appeared to hold back Mahomes in the postseason, when he played in the AFC title match against the Cincinnati Bengals just eight days after suffering a high ankle sprain in the Chiefs' divisional round win.\n",
    "With the game tied and seconds left on the clock in the fourth quarter, he produced a heroic sprint that ultimately helped put kicker Harrison Butker within range.\n",
    "\"I know he was hurting - I know that. He's so mentally tough,\" head coach Andy Reid told reporters at the Super Bowl Opening Night on Monday. \"That run that he made at the end, that was the fastest he's run all year.\"\n",
    "Cornerback James Bradberry said that it would take everything in the Eagles arsenal to stop Mahomes from collecting his second Super Bowl ring.\n",
    "\"You just have to be aware of how dominant he can be. You want to make sure you can contain him, eliminate what he's able to do,\" he told reporters on Tuesday.\n",
    "\"You just want to make sure you put guys in his face. That's what our defensive line has been doing all year.\" \"\"\"\n",
    "\n",
    "sp4 = \"\"\"Los Angeles Lakers forward LeBron James surpassed Kareem Abdul-Jabbar to become the NBA's all-time leading scorer on Tuesday, setting the new mark with a fadeaway jumpshot late in the third quarter of a home game against the Oklahoma City Thunder.\n",
    "'King James', who entered the game needing 36 points to break Abdul-Jabbar’s record of 38,387, sent the sold-out crowd into a frenzy when the ball splashed through the net, raising his arms in triumph as his team mates embraced him.\n",
    "Lakers great Abdul-Jabbar, who took the title from Wilt Chamberlain with his signature skyhook on April 5, 1984, sat courtside at Tuesday's game and stood to applaud James after the record was broken.\n",
    "Play was stopped to recognize the achievement and to let James address the crowd.\n",
    "\"I just want to say thank you to the Laker faithful, you guys are one of a kind,\" James said.\n",
    "\"To be able to be in the presence of such a legend as Kareem is unbelievable, it's very humbling. Please give a standing ovation to 'The Captain.'\"\n",
    "Tributes from his family, U.S. President Joe Biden and students from his \"I Promise School\" were played inside the arena, while NBA Commissioner Adam Silver told Reuters it was an \"historic moment\".\n",
    "\"These types of significant milestones capture the attention of not only basketball fans but broader society,\" Silver said.\n",
    "\"LeBron's pursuit of the scoring record is no exception and billions of people will become aware of this milestone.\"\n",
    "All season long it has been a question of when, not if, James would topple the record. Some thought it may come during Thursday's home game against Milwaukee but James had other ideas.\n",
    "Arriving at the arena in a jet black suit, black shirt and dark sunglasses, James looked like he was going to a funeral.\n",
    "Hours later, he buried Abdul-Jabbar's record.\n",
    "A deafening roar greeted him during the pre-game introductions and another came when he buried a three-pointer five minutes into the opening quarter for his first points of the night.\n",
    "He cut the number he needed to single digits on a straightway three in the second half that sent fans leaping from their seats before the 21-foot, history-making bucket arrived with 10 seconds remaining in the third quarter.\n",
    "\"It's so surreal, because it's something I never made a goal of mine or something I set out to do,\" James said after the game. \"It just happened.\"\n",
    "Drafted into the league as a teenager, the Akron, Ohio native has more than delivered on the massive expectations put on his broad shoulders at a young age.\n",
    "A versatile forward, he helping usher in the era of position-less basketball, winning four titles with three different teams, four MVP awards and four Finals MVP awards.\n",
    "James sits top of the regular season points list followed by Abdul-Jabbar with Utah Jazz great Karl Malone (36,928), late Lakers legend Kobe Bryant (33,643) and Chicago Bulls icon Michael Jordan (32,292) rounding out the top five.\n",
    "\"When I read about the history of the game I never thought that this record would ever be touched,\" James said.\n",
    "\"I just didn't think nobody would have that type of longevity to come out on the floor and play at that level for so long.\n",
    "\"So it's just a complete honor to be a part of this league, to be a part of some of the greats that have ever played this game and to be right at the apex with them.\"\n",
    "Last month, the 38-year-old was named to a record-tying 19th All Star game, a mark also held by Abdul-Jabbar.\n",
    "\"For sure I know I can play a couple more years,\" James said.\n",
    "\"The way I'm feeling, the way my body has been reacting to me throughout the course of this season, I know I can play a couple more years.\n",
    "\"It's all about my mind. My mind is still into it and I am still motivated to go out and try to compete for championships because I feel like that's what I can still do.\"\n",
    "Despite James' historic night, the Lakers fell 133-130 to the Thunder and are now 25-30 on the season.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have scikit-learn installed. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "articles = []\n",
    "for f in [sp0, sp1, sp2, sp3, sp4]:\n",
    "    text = f.replace('\\n', ' ')\n",
    "    articles.append(text)\n",
    "vrizer = CountVectorizer(vocabulary=vocabulary)\n",
    "new_data = vrizer.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Run the classifier from step (1) to predict the classes of the articles in `new_data`. Print out the class predictions.\n",
    "\n",
    "What classes to you think these 5 articles should belong to, based on your own judgement of their content? Can your classifer make an appropriate class prediction for these 5 articles? Analyse the reason for your answers in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data, new_labels, new_class_names, new_vocabulary = \n",
    "y_pred_new = knn_classify(new_data, data, labels, \"cosine\", 3)\n",
    "#  class predictions\n",
    "print(\"Class:\", y_pred_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Introduce a new class, `sport`, to your dataset. The class should contain the 5 articles as above. Add this to your data using the command below. Your new data contains 805 articles, 800 from the original dataset and 5 from the `new_data`, belonging to 5 classes: 200 articles from each of the first 4 classes and 5 articles from the 5th class.\n",
    "\n",
    "Randomly split the new data into a training set containing **100 articles each from 'earn', 'crude', 'trade', and 'interest', and then only 3 articles from 'sport'** (you should be able to use the `sample_indices` function given at the start). Reserve the remaining articles for testing. Test the performance of the new 3-NN classifier.\n",
    "\n",
    "Print the confusion matrix and classification accuracy for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce a new class\n",
    "class_names_augmented = np.append(class_names, 'sport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented = scipy.sparse.vstack((data, new_data))\n",
    "labels_augmented = labels = np.append(labels, np.array([4, 4, 4, 4, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data, training_labels, test_data, test_labels = choose_data(data, labels, 100, 100, 100, 100, 3)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_perform3(times, metric, k, *num_per_class):\n",
    "    # The accuracy of each time saves in a list    \n",
    "    accuracy = []\n",
    "    for i in range(times):\n",
    "        training_data, training_labels, test_data, test_labels = choose_data(data_augmented, labels_augmented, *num_per_class)      \n",
    "        predicted_class = knn_classify(test_data, training_data, training_labels, metric, k)\n",
    "        \n",
    "        # Compare with test labels then decide if it is consist        \n",
    "        accuracy.append(np.sum(predicted_class == test_labels) / len(predicted_class))\n",
    "        \n",
    "        # Draw the confusion matrix        \n",
    "        matrix = confusion_matrix_scratch(test_labels, predicted_class)\n",
    "    return np.array(accuracy), matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_3, matrix3 = knn_perform3(1, \"cosine\", 3, 100, 100, 100, 100, 3)\n",
    "print(\"confusion matrix: \\n\", matrix3)\n",
    "print(\"classification accuracy:\", accuracy_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Repeat the above process 6 times, repeating the random train-test split. For each of the 5 classes, print out its averaged testing accuracy. Comment on your classifier's performance in your report. What are the consequences of having no training data and limited training data for the 'sports' class? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_classification_acuracy(label_number, matrix):\n",
    "    label_classification = np.zeros((2,2), dtype = int)\n",
    "    label_classification[0,0] = matrix[label_number, label_number]\n",
    "    label_classification[1,0] = np.sum(matrix[label_number, np.arange(matrix.shape[1]) != label_number])\n",
    "    label_classification[0,1] = np.sum(matrix[np.arange(matrix.shape[0]) != label_number, label_number])\n",
    "    label_classification[1,1] = np.sum(matrix) - np.sum(label_classification)\n",
    "    \n",
    "    return label_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_perform3_pro(times, metric, k, *num_per_class):\n",
    "    # The accuracy of each time saves in a list    \n",
    "    accuracy = []\n",
    "    accuracy_for_each = []\n",
    "    for i in range(times):\n",
    "        training_data, training_labels, test_data, test_labels = choose_data(data_augmented, labels_augmented, *num_per_class)      \n",
    "        predicted_class = knn_classify(test_data, training_data, training_labels, metric, k)\n",
    "        \n",
    "        # Compare with test labels then decide if it is consist        \n",
    "        accuracy.append(np.sum(predicted_class == test_labels) / len(predicted_class))\n",
    "        \n",
    "        # Draw the confusion matrix        \n",
    "        matrix = confusion_matrix_scratch(test_labels, predicted_class)\n",
    "        \n",
    "        # For each of the 5 classes, print out its averaged testing accuracy\n",
    "        for j in range(5):\n",
    "            print(\"class confusion {}:\\n\".format(j), label_classification_acuracy(j, matrix))\n",
    "            print(\"class accuracy {}:\\n\".format(j), np.sum(np.diag(label_classification_acuracy(j, matrix))) / np.sum(label_classification_acuracy(j, matrix)))\n",
    "            \n",
    "            \n",
    "#     return np.array(accuracy), matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_3p, matrix3p = knn_perform3_pro(6, \"cosine\", 3, 100, 100, 100, 100, 3)\n",
    "# print(\"classification accuracy:\", accuracy_3p)\n",
    "knn_perform3_pro(6, \"cosine\", 3, 100, 100, 100, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Self-learn the concepts of zero-shot learning and few-shot learning. In your report, link these concepts to the experiments you've just performed. Is your model performing zero- or few-shot learning? Explain your reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Result Analysis (4 Marks in Total)\n",
    "\n",
    "### Analysis 1 (2 Marks, Normal)\n",
    "Choose a training-testing trial in Experiment 2 for $k=1$. Observe the testing error of this 1-NN, and estimate the interval where its true error lies with 90% probability. Explain in your report how you compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_perform2A(k, metric, *nums):\n",
    "    training_data, training_labels, test_data, test_labels = choose_data(data, labels, *nums)\n",
    "    predicted_class = knn_classify(test_data, training_data, training_labels, metric, k)\n",
    "\n",
    "    test_err = np.sum(predicted_class != test_labels) / len(predicted_class)\n",
    "    return test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, class_names, vocabulary = np.load(\"ReutersNews_4Classes_sparse.npy\", allow_pickle=True)\n",
    "test_error_1 = knn_perform2A(1, better_metric, 80, 80, 80, 80)\n",
    "# 90% probability\n",
    "Confidence_level_90 = 1.64\n",
    "n = 800\n",
    "alpha = Confidence_level_90 * np.sqrt(test_error_1 * (1 - test_error_1) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Confidence interval: [\",test_error_1-alpha, \",\", test_error_1+alpha, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2 (2 Marks, Normal)\n",
    "The following function `Get_p_value()` is provided to obtain $p$ according to $z_p$. Use this function to perform Analysis 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell first\n",
    "\n",
    "def Get_p_value(zp):\n",
    "    return round(1 - scipy.stats.norm.sf(abs(zp))*2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to compare the output value of function Get_p_value with \n",
    "# the table provided in your lecture notes (e.g., Slide 12, Chapter3C.pdf)\n",
    "\n",
    "print('zp = 0.67, p = ', Get_p_value(0.67))\n",
    "print('zp = 1, p = ', Get_p_value(1))\n",
    "print('zp = 1.64, p = ', Get_p_value(1.64))\n",
    "print('zp = 2.58, p = ', Get_p_value(2.58))\n",
    "print()\n",
    "\n",
    "# you can alert the input zp value and re-run this cell to help you to calculate the corresponding p.\n",
    "print('p = ', Get_p_value(0.43))  \n",
    "\n",
    "\n",
    "# you can change 0.43 to any zp value you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a training-testing trial in Experiment 2 for k=45. Observe the testing error of this 45-NN. Compare it with the 1-NN in Analysis 1. Which one has higher testing sample error? Estimate the probability that it also has higher true error. Explain your answer and how you compute it in the report.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_perform2B(k, metric, *nums):\n",
    "    training_data, training_labels, test_data, test_labels = choose_data(data, labels, *nums)\n",
    "    predicted_class = knn_classify(test_data, training_data, training_labels, metric, k)\n",
    "    test_err = np.sum(predicted_class != test_labels) / len(predicted_class)\n",
    "    \n",
    "    predicted_class = knn_classify(training_data, training_data, training_labels, metric, k)\n",
    "    train_err = np.sum(predicted_class != training_labels) / len(predicted_class)\n",
    "            \n",
    "    return train_err,test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_45 = knn_perform2A(45, better_metric, 80, 80, 80, 80)\n",
    "print(\"testing sample error with k=45 is\", test_error_45)\n",
    "print(\"testing sample error with k=1 is\", test_error_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.sqrt((test_error_45 * (1 - test_error_45) / n) + (test_error_1 * (1 - test_error_1) / n))\n",
    "print(sigma)\n",
    "d = np.abs(test_error_45 - test_error_1)\n",
    "zp = d / sigma \n",
    "\n",
    "# the probability that it also has higher true error.\n",
    "C = 1 - ((1 - Get_p_value(zp) ) / 2)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Selection (4 Marks, Normal)\n",
    "\n",
    "Use your k-NN function with cosine distance. Design an appropriate and complete machine learning experiment, which should include the training, hyper-parameter selection and evaluation stages. In this case, your hyperparameter will be $k$. You can choose from the random subsampling, k-fold CV and LOO approaches for hyperparameter selection. In order to get 4 marks, you should implement this from scrach without using readily implemented data-split functions provided in existing libraries. If you decide to use existing implementation on data splitting, model selection and/or evaluation, you can get at most 2 marks. \n",
    "\n",
    "Explain in the report your strategy for splitting the data, and the design of your chosen hyperparameter selection method. Present your results and chosen value of $k$. Why is it important to split the data into train, test, and validation sets in machine learning experiments? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_fold_CV(data, labels, k, metric, k_range):\n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation to select the best k value for k-NN.\n",
    "\n",
    "    Args:\n",
    "    data: sparse matrix of word occurrences, shape (n_samples, n_features)\n",
    "    labels: array of target labels, shape (n_samples,)\n",
    "    k: number of folds for cross-validation\n",
    "    metric: distance metric to use for k-NN, 'euclidean' or 'cosine'\n",
    "    k_range: range of k values to try, e.g. range(1, 10)\n",
    "\n",
    "    Returns:\n",
    "    best_k: the k value that resulted in the highest mean accuracy across folds\n",
    "    accuracies: list of mean accuracies for each k value in k_range\n",
    "    \"\"\"\n",
    "    fold_indices = np.array_split(np.arange(len(labels)), k)\n",
    "    accuracies = []\n",
    "    for k_val in k_range:\n",
    "        accuracy_sum = 0\n",
    "        for i in range(k):\n",
    "            # Split the data and labels into training and validation sets for this fold\n",
    "            val_indices = fold_indices[i]\n",
    "            train_indices = np.concatenate(fold_indices[:i] + fold_indices[i+1:])\n",
    "            training_data, train_labels = data[train_indices], labels[train_indices]\n",
    "            test_samples, val_labels = data[val_indices], labels[val_indices]\n",
    "            \n",
    "            # Perform k-NN classification on the validation set\n",
    "            predict_y = knn_classify(test_samples, training_data, train_labels, metric, k_val)\n",
    "            \n",
    "            # Calculate accuracy on the validation set and add to the sum for this fold\n",
    "            accuracy = np.sum(predict_y == val_labels) / len(val_labels)\n",
    "            accuracy_sum += accuracy\n",
    "        \n",
    "        # Calculate the mean accuracy across folds for this k value\n",
    "        mean_accuracy = accuracy_sum / k\n",
    "        \n",
    "        # Add the mean accuracy to the list of accuracies for this k value\n",
    "        accuracies.append(mean_accuracy)\n",
    "    \n",
    "    # Find the k value that resulted in the highest mean accuracy across folds\n",
    "    best_k = k_range[np.argmax(accuracies)]\n",
    "    \n",
    "    return best_k, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "metric = 'cosine'\n",
    "k_range = range(1, 10)\n",
    "best_k, accuracies = K_fold_CV(data, labels, k, metric, k_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter selection, Here I use K fold. This involves dividing the data into k equally sized subsets, and then training and testing the model k times, each time using a different subset as the test set and the remaining k-1 subsets as the training set. The results are then averaged to give an overall estimate of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_k, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
